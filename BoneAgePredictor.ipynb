{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittimaxz/Project_BoneAge/blob/main/BoneAgePredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcZjvd_65QVN",
        "outputId": "f60fc71a-ac19-4c7d-a246-f6e84474ff57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Collecting lightning-utilities>=0.4.2\n",
            "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2023.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.13.1+cu116)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.6.0.post0 pytorch-lightning-1.9.0 torchmetrics-0.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "pduFNGhc55Dw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # เชื่อม drive ของเรา ถ้าเชื่อมสำเร็จจะขึ้นคำว่าMounted at /content/drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnsm5FPI6met",
        "outputId": "4c1d4b05-9525-4fd1-f076-f8928d2100cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_path(*rel_path):\n",
        "    return os.path.join('/content/drive/My Drive/Project_Boneage', *rel_path);"
      ],
      "metadata": {
        "id": "8SUUfnwf6qqC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "xGcSme2e-cjG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgeTrainingDataset(Dataset):\n",
        "    def __init__(self,csv_path,img_folder):\n",
        "        self.csv = pd.read_csv(dataset_path(csv_path));\n",
        "        self.img_folder = img_folder;        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.csv.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return transform(Image.open(dataset_path(self.img_folder,str(self.csv['id'][idx])+'.png')).resize((256,256))).double(),\\\n",
        "               torch.from_numpy(np.array(self.csv['boneage'][idx])).double()"
      ],
      "metadata": {
        "id": "yBJUXtkR691X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgeTestingDataset(Dataset):\n",
        "    def __init__(self,csv_path,img_folder):\n",
        "        self.csv = pd.read_csv(dataset_path(csv_path));\n",
        "        self.img_folder = img_folder;        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.csv.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return transform(Image.open(dataset_path(self.img_folder,str(self.csv['Case ID'][idx])+'.png')).resize((256,256))).double(),\\\n",
        "               torch.from_numpy(np.array(self.csv['boneage'][idx])).double()"
      ],
      "metadata": {
        "id": "D-PjS-wI0WxX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BoneAgePredictor, self).__init__()\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        self.batch1 = nn.BatchNorm2d(4)\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(4, 8, 3)\n",
        "        nn.init.kaiming_normal_(self.conv2.weight)\n",
        "        self.batch2 = nn.BatchNorm2d(8)\n",
        "        # Layer 3\n",
        "        self.conv3 = nn.Conv2d(8, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv3.weight)\n",
        "        self.batch3 = nn.BatchNorm2d(16)\n",
        "        # Layer 4\n",
        "        self.conv4 = nn.Conv2d(16, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv4.weight)\n",
        "        self.batch4 = nn.BatchNorm2d(16)\n",
        "        # Layer 5\n",
        "        self.conv5 = nn.Conv2d(16, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv5.weight)\n",
        "        self.batch5 = nn.BatchNorm2d(16)\n",
        "        # Fully connected\n",
        "        self.fc1 = nn.Linear(576, 24)\n",
        "        self.fc2 = nn.Linear(24, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = F.relu(self.batch1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 2\n",
        "        x = F.relu(self.batch2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 3\n",
        "        x = F.relu(self.batch3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 4\n",
        "        x = F.relu(self.batch4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 4\n",
        "        x = F.relu(self.batch5(self.conv5(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Pooling\n",
        "        x = x.view(-1, 576)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WgTn4ZQ3AwYh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.l1_loss(output.view(-1), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "UXdVbnBtFghc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, loader, loader_name):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss += F.l1_loss(output.view(-1), target, reduction='sum').item()  # sum up batch loss            \n",
        "    loss /= len(loader.dataset)\n",
        "    print('\\n', loader_name, 'set: Average loss: {:.4f}\\n'.format(loss))\n",
        "    return loss;"
      ],
      "metadata": {
        "id": "4UbDJqkSGIOY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
        "trainig_data_loader = torch.utils.data.DataLoader(\n",
        "    BoneAgeTrainingDataset('train.csv', 'boneage_training_dataset'),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "testing_data_loader = torch.utils.data.DataLoader(\n",
        "    BoneAgeTestingDataset('boneage-test-dataset.csv', 'boneage-test-dataset'),\n",
        "    batch_size=64, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "ns7Mm3vQG1MH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BoneAgePredictor().double().to(device)\n",
        "print(model)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10, min_lr=1e-6, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPQsyeOTH0ny",
        "outputId": "dff604c8-e051-49db-b489-efd32ce6a4c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoneAgePredictor(\n",
            "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=576, out_features=24, bias=True)\n",
            "  (fc2): Linear(in_features=24, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 4):\n",
        "        train(model, device, trainig_data_loader, optimizer, epoch)\n",
        "        train_loss = test(model, device, trainig_data_loader,'Train')\n",
        "        test_loss = test(model, device, testing_data_loader,'Test')\n",
        "        scheduler.step(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sizddBfdIOdf",
        "outputId": "f207337f-0056-438a-bcbe-1aa1625ef804"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/10089 (0%)]\tLoss: 128.576547\n",
            "Train Epoch: 1 [64/10089 (1%)]\tLoss: 118.559604\n",
            "Train Epoch: 1 [128/10089 (1%)]\tLoss: 130.845862\n",
            "Train Epoch: 1 [192/10089 (2%)]\tLoss: 127.487118\n",
            "Train Epoch: 1 [256/10089 (3%)]\tLoss: 130.859960\n",
            "Train Epoch: 1 [320/10089 (3%)]\tLoss: 122.350478\n",
            "Train Epoch: 1 [384/10089 (4%)]\tLoss: 124.987296\n",
            "Train Epoch: 1 [448/10089 (4%)]\tLoss: 120.399908\n",
            "Train Epoch: 1 [512/10089 (5%)]\tLoss: 108.559722\n",
            "Train Epoch: 1 [576/10089 (6%)]\tLoss: 113.893533\n",
            "Train Epoch: 1 [640/10089 (6%)]\tLoss: 111.671160\n",
            "Train Epoch: 1 [704/10089 (7%)]\tLoss: 120.295775\n",
            "Train Epoch: 1 [768/10089 (8%)]\tLoss: 116.430020\n",
            "Train Epoch: 1 [832/10089 (8%)]\tLoss: 103.292918\n",
            "Train Epoch: 1 [896/10089 (9%)]\tLoss: 111.494374\n",
            "Train Epoch: 1 [960/10089 (9%)]\tLoss: 98.750162\n",
            "Train Epoch: 1 [1024/10089 (10%)]\tLoss: 94.211930\n",
            "Train Epoch: 1 [1088/10089 (11%)]\tLoss: 98.520723\n",
            "Train Epoch: 1 [1152/10089 (11%)]\tLoss: 98.552415\n",
            "Train Epoch: 1 [1216/10089 (12%)]\tLoss: 97.706046\n",
            "Train Epoch: 1 [1280/10089 (13%)]\tLoss: 96.706672\n",
            "Train Epoch: 1 [1344/10089 (13%)]\tLoss: 94.216660\n",
            "Train Epoch: 1 [1408/10089 (14%)]\tLoss: 76.184846\n",
            "Train Epoch: 1 [1472/10089 (15%)]\tLoss: 81.955801\n",
            "Train Epoch: 1 [1536/10089 (15%)]\tLoss: 84.170514\n",
            "Train Epoch: 1 [1600/10089 (16%)]\tLoss: 82.610249\n",
            "Train Epoch: 1 [1664/10089 (16%)]\tLoss: 75.508311\n",
            "Train Epoch: 1 [1728/10089 (17%)]\tLoss: 72.568610\n",
            "Train Epoch: 1 [1792/10089 (18%)]\tLoss: 63.317351\n",
            "Train Epoch: 1 [1856/10089 (18%)]\tLoss: 69.967608\n",
            "Train Epoch: 1 [1920/10089 (19%)]\tLoss: 63.076324\n",
            "Train Epoch: 1 [1984/10089 (20%)]\tLoss: 61.084732\n",
            "Train Epoch: 1 [2048/10089 (20%)]\tLoss: 54.777475\n",
            "Train Epoch: 1 [2112/10089 (21%)]\tLoss: 57.743044\n",
            "Train Epoch: 1 [2176/10089 (22%)]\tLoss: 51.893907\n",
            "Train Epoch: 1 [2240/10089 (22%)]\tLoss: 54.661728\n",
            "Train Epoch: 1 [2304/10089 (23%)]\tLoss: 48.460915\n",
            "Train Epoch: 1 [2368/10089 (23%)]\tLoss: 52.706639\n",
            "Train Epoch: 1 [2432/10089 (24%)]\tLoss: 55.296872\n",
            "Train Epoch: 1 [2496/10089 (25%)]\tLoss: 45.454168\n",
            "Train Epoch: 1 [2560/10089 (25%)]\tLoss: 41.349915\n",
            "Train Epoch: 1 [2624/10089 (26%)]\tLoss: 40.187418\n",
            "Train Epoch: 1 [2688/10089 (27%)]\tLoss: 40.587407\n",
            "Train Epoch: 1 [2752/10089 (27%)]\tLoss: 33.482194\n",
            "Train Epoch: 1 [2816/10089 (28%)]\tLoss: 37.549783\n",
            "Train Epoch: 1 [2880/10089 (28%)]\tLoss: 38.724850\n",
            "Train Epoch: 1 [2944/10089 (29%)]\tLoss: 40.373238\n",
            "Train Epoch: 1 [3008/10089 (30%)]\tLoss: 35.574097\n",
            "Train Epoch: 1 [3072/10089 (30%)]\tLoss: 29.973526\n",
            "Train Epoch: 1 [3136/10089 (31%)]\tLoss: 30.554647\n",
            "Train Epoch: 1 [3200/10089 (32%)]\tLoss: 33.725972\n",
            "Train Epoch: 1 [3264/10089 (32%)]\tLoss: 34.471569\n",
            "Train Epoch: 1 [3328/10089 (33%)]\tLoss: 35.351357\n",
            "Train Epoch: 1 [3392/10089 (34%)]\tLoss: 32.012534\n",
            "Train Epoch: 1 [3456/10089 (34%)]\tLoss: 32.377056\n",
            "Train Epoch: 1 [3520/10089 (35%)]\tLoss: 34.452320\n",
            "Train Epoch: 1 [3584/10089 (35%)]\tLoss: 32.695183\n",
            "Train Epoch: 1 [3648/10089 (36%)]\tLoss: 30.812397\n",
            "Train Epoch: 1 [3712/10089 (37%)]\tLoss: 38.119020\n",
            "Train Epoch: 1 [3776/10089 (37%)]\tLoss: 29.666249\n",
            "Train Epoch: 1 [3840/10089 (38%)]\tLoss: 35.994920\n",
            "Train Epoch: 1 [3904/10089 (39%)]\tLoss: 30.898854\n",
            "Train Epoch: 1 [3968/10089 (39%)]\tLoss: 31.439840\n",
            "Train Epoch: 1 [4032/10089 (40%)]\tLoss: 33.651400\n",
            "Train Epoch: 1 [4096/10089 (41%)]\tLoss: 39.219465\n",
            "Train Epoch: 1 [4160/10089 (41%)]\tLoss: 27.071268\n",
            "Train Epoch: 1 [4224/10089 (42%)]\tLoss: 35.292148\n",
            "Train Epoch: 1 [4288/10089 (42%)]\tLoss: 37.401623\n",
            "Train Epoch: 1 [4352/10089 (43%)]\tLoss: 38.998165\n",
            "Train Epoch: 1 [4416/10089 (44%)]\tLoss: 35.263845\n",
            "Train Epoch: 1 [4480/10089 (44%)]\tLoss: 31.970551\n",
            "Train Epoch: 1 [4544/10089 (45%)]\tLoss: 34.264987\n",
            "Train Epoch: 1 [4608/10089 (46%)]\tLoss: 37.722523\n",
            "Train Epoch: 1 [4672/10089 (46%)]\tLoss: 33.336809\n",
            "Train Epoch: 1 [4736/10089 (47%)]\tLoss: 30.661109\n",
            "Train Epoch: 1 [4800/10089 (47%)]\tLoss: 32.675731\n",
            "Train Epoch: 1 [4864/10089 (48%)]\tLoss: 31.424327\n",
            "Train Epoch: 1 [4928/10089 (49%)]\tLoss: 32.917068\n",
            "Train Epoch: 1 [4992/10089 (49%)]\tLoss: 39.101266\n",
            "Train Epoch: 1 [5056/10089 (50%)]\tLoss: 36.031604\n",
            "Train Epoch: 1 [5120/10089 (51%)]\tLoss: 31.811980\n",
            "Train Epoch: 1 [5184/10089 (51%)]\tLoss: 35.460145\n",
            "Train Epoch: 1 [5248/10089 (52%)]\tLoss: 30.093824\n",
            "Train Epoch: 1 [5312/10089 (53%)]\tLoss: 33.602377\n",
            "Train Epoch: 1 [5376/10089 (53%)]\tLoss: 28.650114\n",
            "Train Epoch: 1 [5440/10089 (54%)]\tLoss: 35.732430\n",
            "Train Epoch: 1 [5504/10089 (54%)]\tLoss: 33.278018\n",
            "Train Epoch: 1 [5568/10089 (55%)]\tLoss: 28.718212\n",
            "Train Epoch: 1 [5632/10089 (56%)]\tLoss: 33.267905\n",
            "Train Epoch: 1 [5696/10089 (56%)]\tLoss: 33.889526\n",
            "Train Epoch: 1 [5760/10089 (57%)]\tLoss: 33.700465\n",
            "Train Epoch: 1 [5824/10089 (58%)]\tLoss: 34.920274\n",
            "Train Epoch: 1 [5888/10089 (58%)]\tLoss: 35.297077\n",
            "Train Epoch: 1 [5952/10089 (59%)]\tLoss: 32.963992\n",
            "Train Epoch: 1 [6016/10089 (59%)]\tLoss: 32.804072\n",
            "Train Epoch: 1 [6080/10089 (60%)]\tLoss: 33.563018\n",
            "Train Epoch: 1 [6144/10089 (61%)]\tLoss: 28.396410\n",
            "Train Epoch: 1 [6208/10089 (61%)]\tLoss: 28.823639\n",
            "Train Epoch: 1 [6272/10089 (62%)]\tLoss: 36.007282\n",
            "Train Epoch: 1 [6336/10089 (63%)]\tLoss: 30.864279\n",
            "Train Epoch: 1 [6400/10089 (63%)]\tLoss: 31.421042\n",
            "Train Epoch: 1 [6464/10089 (64%)]\tLoss: 32.253597\n",
            "Train Epoch: 1 [6528/10089 (65%)]\tLoss: 26.794569\n",
            "Train Epoch: 1 [6592/10089 (65%)]\tLoss: 28.372484\n",
            "Train Epoch: 1 [6656/10089 (66%)]\tLoss: 31.111700\n",
            "Train Epoch: 1 [6720/10089 (66%)]\tLoss: 36.435105\n",
            "Train Epoch: 1 [6784/10089 (67%)]\tLoss: 34.976674\n",
            "Train Epoch: 1 [6848/10089 (68%)]\tLoss: 30.365998\n",
            "Train Epoch: 1 [6912/10089 (68%)]\tLoss: 26.338680\n",
            "Train Epoch: 1 [6976/10089 (69%)]\tLoss: 26.883525\n",
            "Train Epoch: 1 [7040/10089 (70%)]\tLoss: 26.125571\n",
            "Train Epoch: 1 [7104/10089 (70%)]\tLoss: 34.185722\n",
            "Train Epoch: 1 [7168/10089 (71%)]\tLoss: 27.757293\n",
            "Train Epoch: 1 [7232/10089 (72%)]\tLoss: 27.308271\n",
            "Train Epoch: 1 [7296/10089 (72%)]\tLoss: 30.699969\n",
            "Train Epoch: 1 [7360/10089 (73%)]\tLoss: 31.285435\n",
            "Train Epoch: 1 [7424/10089 (73%)]\tLoss: 27.943858\n",
            "Train Epoch: 1 [7488/10089 (74%)]\tLoss: 34.658568\n",
            "Train Epoch: 1 [7552/10089 (75%)]\tLoss: 25.708587\n",
            "Train Epoch: 1 [7616/10089 (75%)]\tLoss: 34.078220\n",
            "Train Epoch: 1 [7680/10089 (76%)]\tLoss: 32.237977\n",
            "Train Epoch: 1 [7744/10089 (77%)]\tLoss: 31.286588\n",
            "Train Epoch: 1 [7808/10089 (77%)]\tLoss: 33.406463\n",
            "Train Epoch: 1 [7872/10089 (78%)]\tLoss: 30.903027\n",
            "Train Epoch: 1 [7936/10089 (78%)]\tLoss: 34.733695\n",
            "Train Epoch: 1 [8000/10089 (79%)]\tLoss: 27.635865\n",
            "Train Epoch: 1 [8064/10089 (80%)]\tLoss: 29.421633\n",
            "Train Epoch: 1 [8128/10089 (80%)]\tLoss: 39.811864\n",
            "Train Epoch: 1 [8192/10089 (81%)]\tLoss: 32.812085\n",
            "Train Epoch: 1 [8256/10089 (82%)]\tLoss: 33.105623\n",
            "Train Epoch: 1 [8320/10089 (82%)]\tLoss: 37.856161\n",
            "Train Epoch: 1 [8384/10089 (83%)]\tLoss: 29.133474\n",
            "Train Epoch: 1 [8448/10089 (84%)]\tLoss: 26.430444\n",
            "Train Epoch: 1 [8512/10089 (84%)]\tLoss: 30.599695\n",
            "Train Epoch: 1 [8576/10089 (85%)]\tLoss: 37.383926\n",
            "Train Epoch: 1 [8640/10089 (85%)]\tLoss: 32.027428\n",
            "Train Epoch: 1 [8704/10089 (86%)]\tLoss: 33.096077\n",
            "Train Epoch: 1 [8768/10089 (87%)]\tLoss: 29.275885\n",
            "Train Epoch: 1 [8832/10089 (87%)]\tLoss: 31.162562\n",
            "Train Epoch: 1 [8896/10089 (88%)]\tLoss: 29.098917\n",
            "Train Epoch: 1 [8960/10089 (89%)]\tLoss: 30.821102\n",
            "Train Epoch: 1 [9024/10089 (89%)]\tLoss: 32.502232\n",
            "Train Epoch: 1 [9088/10089 (90%)]\tLoss: 30.652685\n",
            "Train Epoch: 1 [9152/10089 (91%)]\tLoss: 33.977864\n",
            "Train Epoch: 1 [9216/10089 (91%)]\tLoss: 28.641006\n",
            "Train Epoch: 1 [9280/10089 (92%)]\tLoss: 29.760021\n",
            "Train Epoch: 1 [9344/10089 (92%)]\tLoss: 31.819195\n",
            "Train Epoch: 1 [9408/10089 (93%)]\tLoss: 33.214081\n",
            "Train Epoch: 1 [9472/10089 (94%)]\tLoss: 31.541717\n",
            "Train Epoch: 1 [9536/10089 (94%)]\tLoss: 33.230827\n",
            "Train Epoch: 1 [9600/10089 (95%)]\tLoss: 36.332366\n",
            "Train Epoch: 1 [9664/10089 (96%)]\tLoss: 34.400786\n",
            "Train Epoch: 1 [9728/10089 (96%)]\tLoss: 30.677727\n",
            "Train Epoch: 1 [9792/10089 (97%)]\tLoss: 29.841360\n",
            "Train Epoch: 1 [9856/10089 (97%)]\tLoss: 35.606895\n",
            "Train Epoch: 1 [9920/10089 (98%)]\tLoss: 32.185898\n",
            "Train Epoch: 1 [9984/10089 (99%)]\tLoss: 29.610012\n",
            "Train Epoch: 1 [6437/10089 (99%)]\tLoss: 31.314427\n",
            "\n",
            " Train set: Average loss: 30.3310\n",
            "\n",
            "\n",
            " Test set: Average loss: 33.2527\n",
            "\n",
            "Train Epoch: 2 [0/10089 (0%)]\tLoss: 33.485255\n",
            "Train Epoch: 2 [64/10089 (1%)]\tLoss: 31.904115\n",
            "Train Epoch: 2 [128/10089 (1%)]\tLoss: 29.865134\n",
            "Train Epoch: 2 [192/10089 (2%)]\tLoss: 23.526446\n",
            "Train Epoch: 2 [256/10089 (3%)]\tLoss: 29.953633\n",
            "Train Epoch: 2 [320/10089 (3%)]\tLoss: 28.568149\n",
            "Train Epoch: 2 [384/10089 (4%)]\tLoss: 29.556161\n",
            "Train Epoch: 2 [448/10089 (4%)]\tLoss: 33.060792\n",
            "Train Epoch: 2 [512/10089 (5%)]\tLoss: 34.645837\n",
            "Train Epoch: 2 [576/10089 (6%)]\tLoss: 29.307878\n",
            "Train Epoch: 2 [640/10089 (6%)]\tLoss: 30.644895\n",
            "Train Epoch: 2 [704/10089 (7%)]\tLoss: 34.913257\n",
            "Train Epoch: 2 [768/10089 (8%)]\tLoss: 28.476297\n",
            "Train Epoch: 2 [832/10089 (8%)]\tLoss: 30.311606\n",
            "Train Epoch: 2 [896/10089 (9%)]\tLoss: 31.313766\n",
            "Train Epoch: 2 [960/10089 (9%)]\tLoss: 24.544770\n",
            "Train Epoch: 2 [1024/10089 (10%)]\tLoss: 31.011476\n",
            "Train Epoch: 2 [1088/10089 (11%)]\tLoss: 28.359209\n",
            "Train Epoch: 2 [1152/10089 (11%)]\tLoss: 28.505478\n",
            "Train Epoch: 2 [1216/10089 (12%)]\tLoss: 29.271913\n",
            "Train Epoch: 2 [1280/10089 (13%)]\tLoss: 29.863121\n",
            "Train Epoch: 2 [1344/10089 (13%)]\tLoss: 27.595256\n",
            "Train Epoch: 2 [1408/10089 (14%)]\tLoss: 33.979882\n",
            "Train Epoch: 2 [1472/10089 (15%)]\tLoss: 33.083193\n",
            "Train Epoch: 2 [1536/10089 (15%)]\tLoss: 34.354296\n",
            "Train Epoch: 2 [1600/10089 (16%)]\tLoss: 30.124333\n",
            "Train Epoch: 2 [1664/10089 (16%)]\tLoss: 34.872464\n",
            "Train Epoch: 2 [1728/10089 (17%)]\tLoss: 31.595160\n",
            "Train Epoch: 2 [1792/10089 (18%)]\tLoss: 32.908529\n",
            "Train Epoch: 2 [1856/10089 (18%)]\tLoss: 32.211094\n",
            "Train Epoch: 2 [1920/10089 (19%)]\tLoss: 32.150825\n",
            "Train Epoch: 2 [1984/10089 (20%)]\tLoss: 33.982977\n",
            "Train Epoch: 2 [2048/10089 (20%)]\tLoss: 32.923525\n",
            "Train Epoch: 2 [2112/10089 (21%)]\tLoss: 25.779062\n",
            "Train Epoch: 2 [2176/10089 (22%)]\tLoss: 27.993969\n",
            "Train Epoch: 2 [2240/10089 (22%)]\tLoss: 27.879679\n",
            "Train Epoch: 2 [2304/10089 (23%)]\tLoss: 33.610117\n",
            "Train Epoch: 2 [2368/10089 (23%)]\tLoss: 33.770709\n",
            "Train Epoch: 2 [2432/10089 (24%)]\tLoss: 32.191626\n",
            "Train Epoch: 2 [2496/10089 (25%)]\tLoss: 28.271664\n",
            "Train Epoch: 2 [2560/10089 (25%)]\tLoss: 30.331346\n",
            "Train Epoch: 2 [2624/10089 (26%)]\tLoss: 30.328438\n",
            "Train Epoch: 2 [2688/10089 (27%)]\tLoss: 29.163525\n",
            "Train Epoch: 2 [2752/10089 (27%)]\tLoss: 29.691101\n",
            "Train Epoch: 2 [2816/10089 (28%)]\tLoss: 34.658955\n",
            "Train Epoch: 2 [2880/10089 (28%)]\tLoss: 31.635860\n",
            "Train Epoch: 2 [2944/10089 (29%)]\tLoss: 31.238554\n",
            "Train Epoch: 2 [3008/10089 (30%)]\tLoss: 30.124931\n",
            "Train Epoch: 2 [3072/10089 (30%)]\tLoss: 29.301412\n",
            "Train Epoch: 2 [3136/10089 (31%)]\tLoss: 28.914399\n",
            "Train Epoch: 2 [3200/10089 (32%)]\tLoss: 26.637338\n",
            "Train Epoch: 2 [3264/10089 (32%)]\tLoss: 29.254966\n",
            "Train Epoch: 2 [3328/10089 (33%)]\tLoss: 27.942216\n",
            "Train Epoch: 2 [3392/10089 (34%)]\tLoss: 35.900171\n",
            "Train Epoch: 2 [3456/10089 (34%)]\tLoss: 36.638428\n",
            "Train Epoch: 2 [3520/10089 (35%)]\tLoss: 23.909563\n",
            "Train Epoch: 2 [3584/10089 (35%)]\tLoss: 32.258074\n",
            "Train Epoch: 2 [3648/10089 (36%)]\tLoss: 29.561953\n",
            "Train Epoch: 2 [3712/10089 (37%)]\tLoss: 27.190104\n",
            "Train Epoch: 2 [3776/10089 (37%)]\tLoss: 31.629352\n",
            "Train Epoch: 2 [3840/10089 (38%)]\tLoss: 32.144434\n",
            "Train Epoch: 2 [3904/10089 (39%)]\tLoss: 29.159305\n",
            "Train Epoch: 2 [3968/10089 (39%)]\tLoss: 31.277030\n",
            "Train Epoch: 2 [4032/10089 (40%)]\tLoss: 31.431032\n",
            "Train Epoch: 2 [4096/10089 (41%)]\tLoss: 31.111386\n",
            "Train Epoch: 2 [4160/10089 (41%)]\tLoss: 28.397676\n",
            "Train Epoch: 2 [4224/10089 (42%)]\tLoss: 33.132732\n",
            "Train Epoch: 2 [4288/10089 (42%)]\tLoss: 33.352189\n",
            "Train Epoch: 2 [4352/10089 (43%)]\tLoss: 33.313004\n",
            "Train Epoch: 2 [4416/10089 (44%)]\tLoss: 34.272257\n",
            "Train Epoch: 2 [4480/10089 (44%)]\tLoss: 30.809108\n",
            "Train Epoch: 2 [4544/10089 (45%)]\tLoss: 26.611908\n",
            "Train Epoch: 2 [4608/10089 (46%)]\tLoss: 28.127800\n",
            "Train Epoch: 2 [4672/10089 (46%)]\tLoss: 31.669426\n",
            "Train Epoch: 2 [4736/10089 (47%)]\tLoss: 32.342094\n",
            "Train Epoch: 2 [4800/10089 (47%)]\tLoss: 32.201114\n",
            "Train Epoch: 2 [4864/10089 (48%)]\tLoss: 27.947679\n",
            "Train Epoch: 2 [4928/10089 (49%)]\tLoss: 33.623038\n",
            "Train Epoch: 2 [4992/10089 (49%)]\tLoss: 28.636338\n",
            "Train Epoch: 2 [5056/10089 (50%)]\tLoss: 30.448534\n",
            "Train Epoch: 2 [5120/10089 (51%)]\tLoss: 28.778296\n",
            "Train Epoch: 2 [5184/10089 (51%)]\tLoss: 24.629743\n",
            "Train Epoch: 2 [5248/10089 (52%)]\tLoss: 32.318340\n",
            "Train Epoch: 2 [5312/10089 (53%)]\tLoss: 24.336819\n",
            "Train Epoch: 2 [5376/10089 (53%)]\tLoss: 26.504199\n",
            "Train Epoch: 2 [5440/10089 (54%)]\tLoss: 35.888537\n",
            "Train Epoch: 2 [5504/10089 (54%)]\tLoss: 29.218166\n",
            "Train Epoch: 2 [5568/10089 (55%)]\tLoss: 28.595480\n",
            "Train Epoch: 2 [5632/10089 (56%)]\tLoss: 28.276612\n",
            "Train Epoch: 2 [5696/10089 (56%)]\tLoss: 26.812019\n",
            "Train Epoch: 2 [5760/10089 (57%)]\tLoss: 27.586005\n",
            "Train Epoch: 2 [5824/10089 (58%)]\tLoss: 30.640388\n",
            "Train Epoch: 2 [5888/10089 (58%)]\tLoss: 29.287167\n",
            "Train Epoch: 2 [5952/10089 (59%)]\tLoss: 27.167056\n",
            "Train Epoch: 2 [6016/10089 (59%)]\tLoss: 28.273715\n",
            "Train Epoch: 2 [6080/10089 (60%)]\tLoss: 32.817384\n",
            "Train Epoch: 2 [6144/10089 (61%)]\tLoss: 31.015618\n",
            "Train Epoch: 2 [6208/10089 (61%)]\tLoss: 25.443806\n",
            "Train Epoch: 2 [6272/10089 (62%)]\tLoss: 29.063134\n",
            "Train Epoch: 2 [6336/10089 (63%)]\tLoss: 31.809833\n",
            "Train Epoch: 2 [6400/10089 (63%)]\tLoss: 31.692720\n",
            "Train Epoch: 2 [6464/10089 (64%)]\tLoss: 27.530445\n",
            "Train Epoch: 2 [6528/10089 (65%)]\tLoss: 27.096761\n",
            "Train Epoch: 2 [6592/10089 (65%)]\tLoss: 26.368025\n",
            "Train Epoch: 2 [6656/10089 (66%)]\tLoss: 30.366574\n",
            "Train Epoch: 2 [6720/10089 (66%)]\tLoss: 25.160168\n",
            "Train Epoch: 2 [6784/10089 (67%)]\tLoss: 23.889612\n",
            "Train Epoch: 2 [6848/10089 (68%)]\tLoss: 29.528433\n",
            "Train Epoch: 2 [6912/10089 (68%)]\tLoss: 24.571599\n",
            "Train Epoch: 2 [6976/10089 (69%)]\tLoss: 35.030492\n",
            "Train Epoch: 2 [7040/10089 (70%)]\tLoss: 36.109142\n",
            "Train Epoch: 2 [7104/10089 (70%)]\tLoss: 38.770384\n",
            "Train Epoch: 2 [7168/10089 (71%)]\tLoss: 24.893585\n",
            "Train Epoch: 2 [7232/10089 (72%)]\tLoss: 28.466844\n",
            "Train Epoch: 2 [7296/10089 (72%)]\tLoss: 33.191428\n",
            "Train Epoch: 2 [7360/10089 (73%)]\tLoss: 28.427582\n",
            "Train Epoch: 2 [7424/10089 (73%)]\tLoss: 29.141415\n",
            "Train Epoch: 2 [7488/10089 (74%)]\tLoss: 28.572197\n",
            "Train Epoch: 2 [7552/10089 (75%)]\tLoss: 26.390995\n",
            "Train Epoch: 2 [7616/10089 (75%)]\tLoss: 25.235097\n",
            "Train Epoch: 2 [7680/10089 (76%)]\tLoss: 29.161798\n",
            "Train Epoch: 2 [7744/10089 (77%)]\tLoss: 28.872892\n",
            "Train Epoch: 2 [7808/10089 (77%)]\tLoss: 28.046368\n",
            "Train Epoch: 2 [7872/10089 (78%)]\tLoss: 30.272181\n",
            "Train Epoch: 2 [7936/10089 (78%)]\tLoss: 27.271690\n",
            "Train Epoch: 2 [8000/10089 (79%)]\tLoss: 27.270930\n",
            "Train Epoch: 2 [8064/10089 (80%)]\tLoss: 32.795976\n",
            "Train Epoch: 2 [8128/10089 (80%)]\tLoss: 35.716030\n",
            "Train Epoch: 2 [8192/10089 (81%)]\tLoss: 26.498251\n",
            "Train Epoch: 2 [8256/10089 (82%)]\tLoss: 30.942104\n",
            "Train Epoch: 2 [8320/10089 (82%)]\tLoss: 30.942508\n",
            "Train Epoch: 2 [8384/10089 (83%)]\tLoss: 30.122280\n",
            "Train Epoch: 2 [8448/10089 (84%)]\tLoss: 27.253697\n",
            "Train Epoch: 2 [8512/10089 (84%)]\tLoss: 29.082659\n",
            "Train Epoch: 2 [8576/10089 (85%)]\tLoss: 33.282973\n",
            "Train Epoch: 2 [8640/10089 (85%)]\tLoss: 31.212905\n",
            "Train Epoch: 2 [8704/10089 (86%)]\tLoss: 36.065254\n",
            "Train Epoch: 2 [8768/10089 (87%)]\tLoss: 34.385148\n",
            "Train Epoch: 2 [8832/10089 (87%)]\tLoss: 31.670877\n",
            "Train Epoch: 2 [8896/10089 (88%)]\tLoss: 32.883663\n",
            "Train Epoch: 2 [8960/10089 (89%)]\tLoss: 27.337483\n",
            "Train Epoch: 2 [9024/10089 (89%)]\tLoss: 31.680814\n",
            "Train Epoch: 2 [9088/10089 (90%)]\tLoss: 31.458417\n",
            "Train Epoch: 2 [9152/10089 (91%)]\tLoss: 29.058903\n",
            "Train Epoch: 2 [9216/10089 (91%)]\tLoss: 24.645803\n",
            "Train Epoch: 2 [9280/10089 (92%)]\tLoss: 30.984133\n",
            "Train Epoch: 2 [9344/10089 (92%)]\tLoss: 28.435676\n",
            "Train Epoch: 2 [9408/10089 (93%)]\tLoss: 29.797603\n",
            "Train Epoch: 2 [9472/10089 (94%)]\tLoss: 33.164769\n",
            "Train Epoch: 2 [9536/10089 (94%)]\tLoss: 26.897332\n",
            "Train Epoch: 2 [9600/10089 (95%)]\tLoss: 32.376559\n",
            "Train Epoch: 2 [9664/10089 (96%)]\tLoss: 26.551491\n",
            "Train Epoch: 2 [9728/10089 (96%)]\tLoss: 31.764287\n",
            "Train Epoch: 2 [9792/10089 (97%)]\tLoss: 24.257384\n",
            "Train Epoch: 2 [9856/10089 (97%)]\tLoss: 25.585563\n",
            "Train Epoch: 2 [9920/10089 (98%)]\tLoss: 29.410006\n",
            "Train Epoch: 2 [9984/10089 (99%)]\tLoss: 30.567384\n",
            "Train Epoch: 2 [6437/10089 (99%)]\tLoss: 30.579515\n",
            "\n",
            " Train set: Average loss: 28.8801\n",
            "\n",
            "\n",
            " Test set: Average loss: 33.1210\n",
            "\n",
            "Train Epoch: 3 [0/10089 (0%)]\tLoss: 29.054558\n",
            "Train Epoch: 3 [64/10089 (1%)]\tLoss: 25.523608\n",
            "Train Epoch: 3 [128/10089 (1%)]\tLoss: 36.224146\n",
            "Train Epoch: 3 [192/10089 (2%)]\tLoss: 31.348348\n",
            "Train Epoch: 3 [256/10089 (3%)]\tLoss: 31.927713\n",
            "Train Epoch: 3 [320/10089 (3%)]\tLoss: 32.128695\n",
            "Train Epoch: 3 [384/10089 (4%)]\tLoss: 30.046799\n",
            "Train Epoch: 3 [448/10089 (4%)]\tLoss: 29.266489\n",
            "Train Epoch: 3 [512/10089 (5%)]\tLoss: 27.214427\n",
            "Train Epoch: 3 [576/10089 (6%)]\tLoss: 29.023229\n",
            "Train Epoch: 3 [640/10089 (6%)]\tLoss: 26.019138\n",
            "Train Epoch: 3 [704/10089 (7%)]\tLoss: 31.576151\n",
            "Train Epoch: 3 [768/10089 (8%)]\tLoss: 30.857328\n",
            "Train Epoch: 3 [832/10089 (8%)]\tLoss: 32.461237\n",
            "Train Epoch: 3 [896/10089 (9%)]\tLoss: 28.445234\n",
            "Train Epoch: 3 [960/10089 (9%)]\tLoss: 24.869414\n",
            "Train Epoch: 3 [1024/10089 (10%)]\tLoss: 26.414686\n",
            "Train Epoch: 3 [1088/10089 (11%)]\tLoss: 31.710510\n",
            "Train Epoch: 3 [1152/10089 (11%)]\tLoss: 28.747693\n",
            "Train Epoch: 3 [1216/10089 (12%)]\tLoss: 23.932126\n",
            "Train Epoch: 3 [1280/10089 (13%)]\tLoss: 28.138101\n",
            "Train Epoch: 3 [1344/10089 (13%)]\tLoss: 31.689233\n",
            "Train Epoch: 3 [1408/10089 (14%)]\tLoss: 31.808183\n",
            "Train Epoch: 3 [1472/10089 (15%)]\tLoss: 29.864638\n",
            "Train Epoch: 3 [1536/10089 (15%)]\tLoss: 30.644422\n",
            "Train Epoch: 3 [1600/10089 (16%)]\tLoss: 31.867952\n",
            "Train Epoch: 3 [1664/10089 (16%)]\tLoss: 28.993573\n",
            "Train Epoch: 3 [1728/10089 (17%)]\tLoss: 38.902544\n",
            "Train Epoch: 3 [1792/10089 (18%)]\tLoss: 31.429165\n",
            "Train Epoch: 3 [1856/10089 (18%)]\tLoss: 30.840355\n",
            "Train Epoch: 3 [1920/10089 (19%)]\tLoss: 31.964125\n",
            "Train Epoch: 3 [1984/10089 (20%)]\tLoss: 30.609220\n",
            "Train Epoch: 3 [2048/10089 (20%)]\tLoss: 34.145226\n",
            "Train Epoch: 3 [2112/10089 (21%)]\tLoss: 30.467321\n",
            "Train Epoch: 3 [2176/10089 (22%)]\tLoss: 25.162282\n",
            "Train Epoch: 3 [2240/10089 (22%)]\tLoss: 28.123812\n",
            "Train Epoch: 3 [2304/10089 (23%)]\tLoss: 28.509537\n",
            "Train Epoch: 3 [2368/10089 (23%)]\tLoss: 28.700202\n",
            "Train Epoch: 3 [2432/10089 (24%)]\tLoss: 31.590081\n",
            "Train Epoch: 3 [2496/10089 (25%)]\tLoss: 30.390370\n",
            "Train Epoch: 3 [2560/10089 (25%)]\tLoss: 25.027815\n",
            "Train Epoch: 3 [2624/10089 (26%)]\tLoss: 24.651301\n",
            "Train Epoch: 3 [2688/10089 (27%)]\tLoss: 35.256407\n",
            "Train Epoch: 3 [2752/10089 (27%)]\tLoss: 29.512712\n",
            "Train Epoch: 3 [2816/10089 (28%)]\tLoss: 26.380260\n",
            "Train Epoch: 3 [2880/10089 (28%)]\tLoss: 25.935654\n",
            "Train Epoch: 3 [2944/10089 (29%)]\tLoss: 30.706576\n",
            "Train Epoch: 3 [3008/10089 (30%)]\tLoss: 22.383054\n",
            "Train Epoch: 3 [3072/10089 (30%)]\tLoss: 25.463759\n",
            "Train Epoch: 3 [3136/10089 (31%)]\tLoss: 27.580038\n",
            "Train Epoch: 3 [3200/10089 (32%)]\tLoss: 26.192857\n",
            "Train Epoch: 3 [3264/10089 (32%)]\tLoss: 26.049267\n",
            "Train Epoch: 3 [3328/10089 (33%)]\tLoss: 29.344660\n",
            "Train Epoch: 3 [3392/10089 (34%)]\tLoss: 27.405620\n",
            "Train Epoch: 3 [3456/10089 (34%)]\tLoss: 27.757296\n",
            "Train Epoch: 3 [3520/10089 (35%)]\tLoss: 29.191875\n",
            "Train Epoch: 3 [3584/10089 (35%)]\tLoss: 27.967189\n",
            "Train Epoch: 3 [3648/10089 (36%)]\tLoss: 29.436703\n",
            "Train Epoch: 3 [3712/10089 (37%)]\tLoss: 29.982763\n",
            "Train Epoch: 3 [3776/10089 (37%)]\tLoss: 25.984313\n",
            "Train Epoch: 3 [3840/10089 (38%)]\tLoss: 35.701698\n",
            "Train Epoch: 3 [3904/10089 (39%)]\tLoss: 27.448793\n",
            "Train Epoch: 3 [3968/10089 (39%)]\tLoss: 28.828502\n",
            "Train Epoch: 3 [4032/10089 (40%)]\tLoss: 34.222889\n",
            "Train Epoch: 3 [4096/10089 (41%)]\tLoss: 29.341832\n",
            "Train Epoch: 3 [4160/10089 (41%)]\tLoss: 29.199085\n",
            "Train Epoch: 3 [4224/10089 (42%)]\tLoss: 23.981189\n",
            "Train Epoch: 3 [4288/10089 (42%)]\tLoss: 30.866344\n",
            "Train Epoch: 3 [4352/10089 (43%)]\tLoss: 28.404990\n",
            "Train Epoch: 3 [4416/10089 (44%)]\tLoss: 28.492477\n",
            "Train Epoch: 3 [4480/10089 (44%)]\tLoss: 25.738986\n",
            "Train Epoch: 3 [4544/10089 (45%)]\tLoss: 29.069592\n",
            "Train Epoch: 3 [4608/10089 (46%)]\tLoss: 26.600655\n",
            "Train Epoch: 3 [4672/10089 (46%)]\tLoss: 24.164935\n",
            "Train Epoch: 3 [4736/10089 (47%)]\tLoss: 27.226808\n",
            "Train Epoch: 3 [4800/10089 (47%)]\tLoss: 24.038112\n",
            "Train Epoch: 3 [4864/10089 (48%)]\tLoss: 25.671999\n",
            "Train Epoch: 3 [4928/10089 (49%)]\tLoss: 27.531176\n",
            "Train Epoch: 3 [4992/10089 (49%)]\tLoss: 25.413063\n",
            "Train Epoch: 3 [5056/10089 (50%)]\tLoss: 29.466427\n",
            "Train Epoch: 3 [5120/10089 (51%)]\tLoss: 29.760875\n",
            "Train Epoch: 3 [5184/10089 (51%)]\tLoss: 33.258345\n",
            "Train Epoch: 3 [5248/10089 (52%)]\tLoss: 24.042086\n",
            "Train Epoch: 3 [5312/10089 (53%)]\tLoss: 31.410409\n",
            "Train Epoch: 3 [5376/10089 (53%)]\tLoss: 31.649827\n",
            "Train Epoch: 3 [5440/10089 (54%)]\tLoss: 25.606954\n",
            "Train Epoch: 3 [5504/10089 (54%)]\tLoss: 25.749661\n",
            "Train Epoch: 3 [5568/10089 (55%)]\tLoss: 26.397705\n",
            "Train Epoch: 3 [5632/10089 (56%)]\tLoss: 30.554112\n",
            "Train Epoch: 3 [5696/10089 (56%)]\tLoss: 30.643339\n",
            "Train Epoch: 3 [5760/10089 (57%)]\tLoss: 34.427658\n",
            "Train Epoch: 3 [5824/10089 (58%)]\tLoss: 27.880119\n",
            "Train Epoch: 3 [5888/10089 (58%)]\tLoss: 29.948870\n",
            "Train Epoch: 3 [5952/10089 (59%)]\tLoss: 29.166549\n",
            "Train Epoch: 3 [6016/10089 (59%)]\tLoss: 28.488502\n",
            "Train Epoch: 3 [6080/10089 (60%)]\tLoss: 24.959961\n",
            "Train Epoch: 3 [6144/10089 (61%)]\tLoss: 27.289954\n",
            "Train Epoch: 3 [6208/10089 (61%)]\tLoss: 24.383986\n",
            "Train Epoch: 3 [6272/10089 (62%)]\tLoss: 28.324133\n",
            "Train Epoch: 3 [6336/10089 (63%)]\tLoss: 26.550710\n",
            "Train Epoch: 3 [6400/10089 (63%)]\tLoss: 33.973100\n",
            "Train Epoch: 3 [6464/10089 (64%)]\tLoss: 27.015612\n",
            "Train Epoch: 3 [6528/10089 (65%)]\tLoss: 23.209185\n",
            "Train Epoch: 3 [6592/10089 (65%)]\tLoss: 27.904998\n",
            "Train Epoch: 3 [6656/10089 (66%)]\tLoss: 28.335019\n",
            "Train Epoch: 3 [6720/10089 (66%)]\tLoss: 29.284647\n",
            "Train Epoch: 3 [6784/10089 (67%)]\tLoss: 25.160256\n",
            "Train Epoch: 3 [6848/10089 (68%)]\tLoss: 30.303362\n",
            "Train Epoch: 3 [6912/10089 (68%)]\tLoss: 34.290372\n",
            "Train Epoch: 3 [6976/10089 (69%)]\tLoss: 32.488531\n",
            "Train Epoch: 3 [7040/10089 (70%)]\tLoss: 25.479713\n",
            "Train Epoch: 3 [7104/10089 (70%)]\tLoss: 25.541539\n",
            "Train Epoch: 3 [7168/10089 (71%)]\tLoss: 34.233132\n",
            "Train Epoch: 3 [7232/10089 (72%)]\tLoss: 25.245829\n",
            "Train Epoch: 3 [7296/10089 (72%)]\tLoss: 26.906465\n",
            "Train Epoch: 3 [7360/10089 (73%)]\tLoss: 24.271792\n",
            "Train Epoch: 3 [7424/10089 (73%)]\tLoss: 31.023725\n",
            "Train Epoch: 3 [7488/10089 (74%)]\tLoss: 31.323847\n",
            "Train Epoch: 3 [7552/10089 (75%)]\tLoss: 31.258527\n",
            "Train Epoch: 3 [7616/10089 (75%)]\tLoss: 28.869065\n",
            "Train Epoch: 3 [7680/10089 (76%)]\tLoss: 27.241803\n",
            "Train Epoch: 3 [7744/10089 (77%)]\tLoss: 32.809939\n",
            "Train Epoch: 3 [7808/10089 (77%)]\tLoss: 31.646888\n",
            "Train Epoch: 3 [7872/10089 (78%)]\tLoss: 35.288522\n",
            "Train Epoch: 3 [7936/10089 (78%)]\tLoss: 31.697899\n",
            "Train Epoch: 3 [8000/10089 (79%)]\tLoss: 27.561811\n",
            "Train Epoch: 3 [8064/10089 (80%)]\tLoss: 33.726320\n",
            "Train Epoch: 3 [8128/10089 (80%)]\tLoss: 32.207668\n",
            "Train Epoch: 3 [8192/10089 (81%)]\tLoss: 28.710374\n",
            "Train Epoch: 3 [8256/10089 (82%)]\tLoss: 24.215820\n",
            "Train Epoch: 3 [8320/10089 (82%)]\tLoss: 23.156330\n",
            "Train Epoch: 3 [8384/10089 (83%)]\tLoss: 27.616225\n",
            "Train Epoch: 3 [8448/10089 (84%)]\tLoss: 31.524238\n",
            "Train Epoch: 3 [8512/10089 (84%)]\tLoss: 30.429012\n",
            "Train Epoch: 3 [8576/10089 (85%)]\tLoss: 29.243367\n",
            "Train Epoch: 3 [8640/10089 (85%)]\tLoss: 27.890952\n",
            "Train Epoch: 3 [8704/10089 (86%)]\tLoss: 30.548020\n",
            "Train Epoch: 3 [8768/10089 (87%)]\tLoss: 28.503199\n",
            "Train Epoch: 3 [8832/10089 (87%)]\tLoss: 26.576316\n",
            "Train Epoch: 3 [8896/10089 (88%)]\tLoss: 27.686780\n",
            "Train Epoch: 3 [8960/10089 (89%)]\tLoss: 28.134796\n",
            "Train Epoch: 3 [9024/10089 (89%)]\tLoss: 28.404785\n",
            "Train Epoch: 3 [9088/10089 (90%)]\tLoss: 27.645900\n",
            "Train Epoch: 3 [9152/10089 (91%)]\tLoss: 30.202023\n",
            "Train Epoch: 3 [9216/10089 (91%)]\tLoss: 27.078005\n",
            "Train Epoch: 3 [9280/10089 (92%)]\tLoss: 25.444152\n",
            "Train Epoch: 3 [9344/10089 (92%)]\tLoss: 26.581045\n",
            "Train Epoch: 3 [9408/10089 (93%)]\tLoss: 33.050134\n",
            "Train Epoch: 3 [9472/10089 (94%)]\tLoss: 22.483881\n",
            "Train Epoch: 3 [9536/10089 (94%)]\tLoss: 27.476975\n",
            "Train Epoch: 3 [9600/10089 (95%)]\tLoss: 29.610375\n",
            "Train Epoch: 3 [9664/10089 (96%)]\tLoss: 30.737048\n",
            "Train Epoch: 3 [9728/10089 (96%)]\tLoss: 26.787987\n",
            "Train Epoch: 3 [9792/10089 (97%)]\tLoss: 28.092447\n",
            "Train Epoch: 3 [9856/10089 (97%)]\tLoss: 28.794255\n",
            "Train Epoch: 3 [9920/10089 (98%)]\tLoss: 31.133069\n",
            "Train Epoch: 3 [9984/10089 (99%)]\tLoss: 27.856907\n",
            "Train Epoch: 3 [6437/10089 (99%)]\tLoss: 27.475344\n",
            "\n",
            " Train set: Average loss: 28.7285\n",
            "\n",
            "\n",
            " Test set: Average loss: 32.1605\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check = torch.rand(5,1,256,256)\n",
        "# model = BoneAgePredictor();\n",
        "# print(model.forward(check).size())"
      ],
      "metadata": {
        "id": "JV1tLDbUCw0Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check = pd.read_csv(dataset_path('boneage-training-dataset.csv'));\n",
        "# print(check)\n",
        "# print(check['boneage'][0])\n",
        "# print(check['id'][1])\n",
        "# check = Image.open(dataset_path('boneage-training-dataset',str(check['id'][1])+'.png')).resize((256,256))\n",
        "# print(check)\n",
        "# # check = transform(check)\n",
        "# # print(check.size())\n",
        "# # check = torch.from_numpy(np.array(check['boneage'][0]))\n",
        "# # print(check)"
      ],
      "metadata": {
        "id": "W1zPxAL38T_w"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}