{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kittimaxz/Project_BoneAge/blob/main/BoneAgePredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pduFNGhc55Dw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # เชื่อม drive ของเรา ถ้าเชื่อมสำเร็จจะขึ้นคำว่าMounted at /content/drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnsm5FPI6met",
        "outputId": "edcef4de-cdaa-4fc9-ba81-cff88f132c7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_path(*rel_path):\n",
        "    return os.path.join('/content/drive/My Drive/Project_Boneage', *rel_path);"
      ],
      "metadata": {
        "id": "8SUUfnwf6qqC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "xGcSme2e-cjG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgeTrainingDataset(Dataset):\n",
        "    def __init__(self,csv_path,img_folder):\n",
        "        self.csv = pd.read_csv(dataset_path(csv_path));\n",
        "        self.img_folder = img_folder;        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.csv.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return transform(Image.open(dataset_path(self.img_folder,str(self.csv['id'][idx])+'.png')).resize((256,256))).double(),\\\n",
        "               torch.from_numpy(np.array(self.csv['boneage zscore'][idx])).double()"
      ],
      "metadata": {
        "id": "yBJUXtkR691X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgeValidatingDataset(Dataset):\n",
        "    def __init__(self,csv_path,img_folder):\n",
        "        self.csv = pd.read_csv(dataset_path(csv_path));\n",
        "        self.img_folder = img_folder;        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.csv.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return transform(Image.open(dataset_path(self.img_folder,str(self.csv['id'][idx])+'.png')).resize((256,256))).double(),\\\n",
        "               torch.from_numpy(np.array(self.csv['boneage zscore'][idx])).double()"
      ],
      "metadata": {
        "id": "D-PjS-wI0WxX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoneAgePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BoneAgePredictor, self).__init__()\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        self.batch1 = nn.BatchNorm2d(4)\n",
        "        # Layer 2\n",
        "        self.conv2 = nn.Conv2d(4, 8, 3)\n",
        "        nn.init.kaiming_normal_(self.conv2.weight)\n",
        "        self.batch2 = nn.BatchNorm2d(8)\n",
        "        # Layer 3\n",
        "        self.conv3 = nn.Conv2d(8, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv3.weight)\n",
        "        self.batch3 = nn.BatchNorm2d(16)\n",
        "        # Layer 4\n",
        "        self.conv4 = nn.Conv2d(16, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv4.weight)\n",
        "        self.batch4 = nn.BatchNorm2d(16)\n",
        "        # Layer 5\n",
        "        self.conv5 = nn.Conv2d(16, 16, 3)\n",
        "        nn.init.kaiming_normal_(self.conv5.weight)\n",
        "        self.batch5 = nn.BatchNorm2d(16)\n",
        "        # Fully connected\n",
        "        self.fc1 = nn.Linear(576, 24)\n",
        "        self.fc2 = nn.Linear(24, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = F.relu(self.batch1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 2\n",
        "        x = F.relu(self.batch2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 3\n",
        "        x = F.relu(self.batch3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 4\n",
        "        x = F.relu(self.batch4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Layer 5\n",
        "        x = F.relu(self.batch5(self.conv5(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        # Pooling\n",
        "        x = x.view(-1, 576)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WgTn4ZQ3AwYh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.l1_loss(output.view(-1), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "UXdVbnBtFghc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(model, device, loader, loader_name):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss += F.l1_loss(output.view(-1), target, reduction='sum').item()  # sum up batch loss            \n",
        "    loss /= len(loader.dataset)\n",
        "    print('\\n', loader_name, 'set: Average loss: {:.4f}\\n'.format(loss))\n",
        "    return loss;"
      ],
      "metadata": {
        "id": "4UbDJqkSGIOY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
        "trainig_data_loader = torch.utils.data.DataLoader(\n",
        "    BoneAgeTrainingDataset('train_z.csv', 'boneage_training_dataset'),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "validating_data_loader = torch.utils.data.DataLoader(\n",
        "    BoneAgeValidatingDataset('validation_z.csv', 'boneage_validation_dataset'),\n",
        "    batch_size=64, shuffle=True, **kwargs)"
      ],
      "metadata": {
        "id": "ns7Mm3vQG1MH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BoneAgePredictor().double().to(device)\n",
        "print(model)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=2, min_lr=1e-3, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPQsyeOTH0ny",
        "outputId": "db0fbb07-6871-4cfb-99b6-711b37b0b3c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoneAgePredictor(\n",
            "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=576, out_features=24, bias=True)\n",
            "  (fc2): Linear(in_features=24, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 4):\n",
        "        train(model, device, trainig_data_loader, optimizer, epoch)\n",
        "        train_loss = val(model, device, trainig_data_loader,'Train')\n",
        "        val_loss = val(model, device, validating_data_loader,'Validate')\n",
        "        scheduler.step(val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sizddBfdIOdf",
        "outputId": "5e8a0219-6347-4db5-b677-b7a8c5c0ba07"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/10089 (0%)]\tLoss: 0.832555\n",
            "Train Epoch: 1 [64/10089 (1%)]\tLoss: 0.857574\n",
            "Train Epoch: 1 [128/10089 (1%)]\tLoss: 0.932253\n",
            "Train Epoch: 1 [192/10089 (2%)]\tLoss: 0.945262\n",
            "Train Epoch: 1 [256/10089 (3%)]\tLoss: 0.930528\n",
            "Train Epoch: 1 [320/10089 (3%)]\tLoss: 0.770628\n",
            "Train Epoch: 1 [384/10089 (4%)]\tLoss: 0.855036\n",
            "Train Epoch: 1 [448/10089 (4%)]\tLoss: 0.845746\n",
            "Train Epoch: 1 [512/10089 (5%)]\tLoss: 0.784871\n",
            "Train Epoch: 1 [576/10089 (6%)]\tLoss: 0.787687\n",
            "Train Epoch: 1 [640/10089 (6%)]\tLoss: 0.877971\n",
            "Train Epoch: 1 [704/10089 (7%)]\tLoss: 0.730117\n",
            "Train Epoch: 1 [768/10089 (8%)]\tLoss: 0.554744\n",
            "Train Epoch: 1 [832/10089 (8%)]\tLoss: 0.780711\n",
            "Train Epoch: 1 [896/10089 (9%)]\tLoss: 0.811632\n",
            "Train Epoch: 1 [960/10089 (9%)]\tLoss: 0.801401\n",
            "Train Epoch: 1 [1024/10089 (10%)]\tLoss: 0.799214\n",
            "Train Epoch: 1 [1088/10089 (11%)]\tLoss: 0.733927\n",
            "Train Epoch: 1 [1152/10089 (11%)]\tLoss: 0.650201\n",
            "Train Epoch: 1 [1216/10089 (12%)]\tLoss: 0.781102\n",
            "Train Epoch: 1 [1280/10089 (13%)]\tLoss: 0.816013\n",
            "Train Epoch: 1 [1344/10089 (13%)]\tLoss: 0.768758\n",
            "Train Epoch: 1 [1408/10089 (14%)]\tLoss: 0.836500\n",
            "Train Epoch: 1 [1472/10089 (15%)]\tLoss: 0.756329\n",
            "Train Epoch: 1 [1536/10089 (15%)]\tLoss: 0.817819\n",
            "Train Epoch: 1 [1600/10089 (16%)]\tLoss: 0.736247\n",
            "Train Epoch: 1 [1664/10089 (16%)]\tLoss: 0.728409\n",
            "Train Epoch: 1 [1728/10089 (17%)]\tLoss: 0.713744\n",
            "Train Epoch: 1 [1792/10089 (18%)]\tLoss: 0.836115\n",
            "Train Epoch: 1 [1856/10089 (18%)]\tLoss: 0.766805\n",
            "Train Epoch: 1 [1920/10089 (19%)]\tLoss: 0.819131\n",
            "Train Epoch: 1 [1984/10089 (20%)]\tLoss: 0.839952\n",
            "Train Epoch: 1 [2048/10089 (20%)]\tLoss: 0.750363\n",
            "Train Epoch: 1 [2112/10089 (21%)]\tLoss: 0.723092\n",
            "Train Epoch: 1 [2176/10089 (22%)]\tLoss: 0.711245\n",
            "Train Epoch: 1 [2240/10089 (22%)]\tLoss: 0.699768\n",
            "Train Epoch: 1 [2304/10089 (23%)]\tLoss: 0.692800\n",
            "Train Epoch: 1 [2368/10089 (23%)]\tLoss: 0.920064\n",
            "Train Epoch: 1 [2432/10089 (24%)]\tLoss: 0.723199\n",
            "Train Epoch: 1 [2496/10089 (25%)]\tLoss: 0.706962\n",
            "Train Epoch: 1 [2560/10089 (25%)]\tLoss: 0.632324\n",
            "Train Epoch: 1 [2624/10089 (26%)]\tLoss: 0.601023\n",
            "Train Epoch: 1 [2688/10089 (27%)]\tLoss: 0.670797\n",
            "Train Epoch: 1 [2752/10089 (27%)]\tLoss: 0.745367\n",
            "Train Epoch: 1 [2816/10089 (28%)]\tLoss: 0.728345\n",
            "Train Epoch: 1 [2880/10089 (28%)]\tLoss: 0.793617\n",
            "Train Epoch: 1 [2944/10089 (29%)]\tLoss: 0.697905\n",
            "Train Epoch: 1 [3008/10089 (30%)]\tLoss: 0.803252\n",
            "Train Epoch: 1 [3072/10089 (30%)]\tLoss: 0.732071\n",
            "Train Epoch: 1 [3136/10089 (31%)]\tLoss: 0.835266\n",
            "Train Epoch: 1 [3200/10089 (32%)]\tLoss: 0.709439\n",
            "Train Epoch: 1 [3264/10089 (32%)]\tLoss: 0.739340\n",
            "Train Epoch: 1 [3328/10089 (33%)]\tLoss: 0.608047\n",
            "Train Epoch: 1 [3392/10089 (34%)]\tLoss: 0.784944\n",
            "Train Epoch: 1 [3456/10089 (34%)]\tLoss: 0.754066\n",
            "Train Epoch: 1 [3520/10089 (35%)]\tLoss: 0.657068\n",
            "Train Epoch: 1 [3584/10089 (35%)]\tLoss: 0.702821\n",
            "Train Epoch: 1 [3648/10089 (36%)]\tLoss: 0.794320\n",
            "Train Epoch: 1 [3712/10089 (37%)]\tLoss: 0.661277\n",
            "Train Epoch: 1 [3776/10089 (37%)]\tLoss: 0.742053\n",
            "Train Epoch: 1 [3840/10089 (38%)]\tLoss: 0.638161\n",
            "Train Epoch: 1 [3904/10089 (39%)]\tLoss: 0.969369\n",
            "Train Epoch: 1 [3968/10089 (39%)]\tLoss: 0.752959\n",
            "Train Epoch: 1 [4032/10089 (40%)]\tLoss: 0.790303\n",
            "Train Epoch: 1 [4096/10089 (41%)]\tLoss: 0.695938\n",
            "Train Epoch: 1 [4160/10089 (41%)]\tLoss: 0.705799\n",
            "Train Epoch: 1 [4224/10089 (42%)]\tLoss: 0.679830\n",
            "Train Epoch: 1 [4288/10089 (42%)]\tLoss: 0.720844\n",
            "Train Epoch: 1 [4352/10089 (43%)]\tLoss: 0.558080\n",
            "Train Epoch: 1 [4416/10089 (44%)]\tLoss: 0.665698\n",
            "Train Epoch: 1 [4480/10089 (44%)]\tLoss: 0.715463\n",
            "Train Epoch: 1 [4544/10089 (45%)]\tLoss: 0.647967\n",
            "Train Epoch: 1 [4608/10089 (46%)]\tLoss: 0.672719\n",
            "Train Epoch: 1 [4672/10089 (46%)]\tLoss: 0.702274\n",
            "Train Epoch: 1 [4736/10089 (47%)]\tLoss: 0.655384\n",
            "Train Epoch: 1 [4800/10089 (47%)]\tLoss: 0.607564\n",
            "Train Epoch: 1 [4864/10089 (48%)]\tLoss: 0.708541\n",
            "Train Epoch: 1 [4928/10089 (49%)]\tLoss: 0.817690\n",
            "Train Epoch: 1 [4992/10089 (49%)]\tLoss: 0.567986\n",
            "Train Epoch: 1 [5056/10089 (50%)]\tLoss: 0.661047\n",
            "Train Epoch: 1 [5120/10089 (51%)]\tLoss: 0.742277\n",
            "Train Epoch: 1 [5184/10089 (51%)]\tLoss: 0.698342\n",
            "Train Epoch: 1 [5248/10089 (52%)]\tLoss: 0.856346\n",
            "Train Epoch: 1 [5312/10089 (53%)]\tLoss: 0.599591\n",
            "Train Epoch: 1 [5376/10089 (53%)]\tLoss: 0.563447\n",
            "Train Epoch: 1 [5440/10089 (54%)]\tLoss: 0.611661\n",
            "Train Epoch: 1 [5504/10089 (54%)]\tLoss: 0.717575\n",
            "Train Epoch: 1 [5568/10089 (55%)]\tLoss: 0.736096\n",
            "Train Epoch: 1 [5632/10089 (56%)]\tLoss: 0.647053\n",
            "Train Epoch: 1 [5696/10089 (56%)]\tLoss: 0.663455\n",
            "Train Epoch: 1 [5760/10089 (57%)]\tLoss: 0.783204\n",
            "Train Epoch: 1 [5824/10089 (58%)]\tLoss: 0.724765\n",
            "Train Epoch: 1 [5888/10089 (58%)]\tLoss: 0.820177\n",
            "Train Epoch: 1 [5952/10089 (59%)]\tLoss: 0.614040\n",
            "Train Epoch: 1 [6016/10089 (59%)]\tLoss: 0.682201\n",
            "Train Epoch: 1 [6080/10089 (60%)]\tLoss: 0.620670\n",
            "Train Epoch: 1 [6144/10089 (61%)]\tLoss: 0.686094\n",
            "Train Epoch: 1 [6208/10089 (61%)]\tLoss: 0.656679\n",
            "Train Epoch: 1 [6272/10089 (62%)]\tLoss: 0.605859\n",
            "Train Epoch: 1 [6336/10089 (63%)]\tLoss: 0.636877\n",
            "Train Epoch: 1 [6400/10089 (63%)]\tLoss: 0.665451\n",
            "Train Epoch: 1 [6464/10089 (64%)]\tLoss: 0.637861\n",
            "Train Epoch: 1 [6528/10089 (65%)]\tLoss: 0.640324\n",
            "Train Epoch: 1 [6592/10089 (65%)]\tLoss: 0.570632\n",
            "Train Epoch: 1 [6656/10089 (66%)]\tLoss: 0.656821\n",
            "Train Epoch: 1 [6720/10089 (66%)]\tLoss: 0.607049\n",
            "Train Epoch: 1 [6784/10089 (67%)]\tLoss: 0.619900\n",
            "Train Epoch: 1 [6848/10089 (68%)]\tLoss: 0.674884\n",
            "Train Epoch: 1 [6912/10089 (68%)]\tLoss: 0.679817\n",
            "Train Epoch: 1 [6976/10089 (69%)]\tLoss: 0.658665\n",
            "Train Epoch: 1 [7040/10089 (70%)]\tLoss: 0.679990\n",
            "Train Epoch: 1 [7104/10089 (70%)]\tLoss: 0.560791\n",
            "Train Epoch: 1 [7168/10089 (71%)]\tLoss: 0.537242\n",
            "Train Epoch: 1 [7232/10089 (72%)]\tLoss: 0.619493\n",
            "Train Epoch: 1 [7296/10089 (72%)]\tLoss: 0.664466\n",
            "Train Epoch: 1 [7360/10089 (73%)]\tLoss: 0.639490\n",
            "Train Epoch: 1 [7424/10089 (73%)]\tLoss: 0.551288\n",
            "Train Epoch: 1 [7488/10089 (74%)]\tLoss: 0.663601\n",
            "Train Epoch: 1 [7552/10089 (75%)]\tLoss: 0.529343\n",
            "Train Epoch: 1 [7616/10089 (75%)]\tLoss: 0.700396\n",
            "Train Epoch: 1 [7680/10089 (76%)]\tLoss: 0.631980\n",
            "Train Epoch: 1 [7744/10089 (77%)]\tLoss: 0.667729\n",
            "Train Epoch: 1 [7808/10089 (77%)]\tLoss: 0.563832\n",
            "Train Epoch: 1 [7872/10089 (78%)]\tLoss: 0.470499\n",
            "Train Epoch: 1 [7936/10089 (78%)]\tLoss: 0.691319\n",
            "Train Epoch: 1 [8000/10089 (79%)]\tLoss: 0.634072\n",
            "Train Epoch: 1 [8064/10089 (80%)]\tLoss: 0.618272\n",
            "Train Epoch: 1 [8128/10089 (80%)]\tLoss: 0.702941\n",
            "Train Epoch: 1 [8192/10089 (81%)]\tLoss: 0.668709\n",
            "Train Epoch: 1 [8256/10089 (82%)]\tLoss: 0.629492\n",
            "Train Epoch: 1 [8320/10089 (82%)]\tLoss: 0.594245\n",
            "Train Epoch: 1 [8384/10089 (83%)]\tLoss: 0.735356\n",
            "Train Epoch: 1 [8448/10089 (84%)]\tLoss: 0.706285\n",
            "Train Epoch: 1 [8512/10089 (84%)]\tLoss: 0.750713\n",
            "Train Epoch: 1 [8576/10089 (85%)]\tLoss: 0.870757\n",
            "Train Epoch: 1 [8640/10089 (85%)]\tLoss: 0.686922\n",
            "Train Epoch: 1 [8704/10089 (86%)]\tLoss: 0.603462\n",
            "Train Epoch: 1 [8768/10089 (87%)]\tLoss: 0.729766\n",
            "Train Epoch: 1 [8832/10089 (87%)]\tLoss: 0.678238\n",
            "Train Epoch: 1 [8896/10089 (88%)]\tLoss: 0.638550\n",
            "Train Epoch: 1 [8960/10089 (89%)]\tLoss: 0.567937\n",
            "Train Epoch: 1 [9024/10089 (89%)]\tLoss: 0.483425\n",
            "Train Epoch: 1 [9088/10089 (90%)]\tLoss: 0.623758\n",
            "Train Epoch: 1 [9152/10089 (91%)]\tLoss: 0.713417\n",
            "Train Epoch: 1 [9216/10089 (91%)]\tLoss: 0.683541\n",
            "Train Epoch: 1 [9280/10089 (92%)]\tLoss: 0.605799\n",
            "Train Epoch: 1 [9344/10089 (92%)]\tLoss: 0.627838\n",
            "Train Epoch: 1 [9408/10089 (93%)]\tLoss: 0.658389\n",
            "Train Epoch: 1 [9472/10089 (94%)]\tLoss: 0.717386\n",
            "Train Epoch: 1 [9536/10089 (94%)]\tLoss: 0.683652\n",
            "Train Epoch: 1 [9600/10089 (95%)]\tLoss: 0.644312\n",
            "Train Epoch: 1 [9664/10089 (96%)]\tLoss: 0.750693\n",
            "Train Epoch: 1 [9728/10089 (96%)]\tLoss: 0.729975\n",
            "Train Epoch: 1 [9792/10089 (97%)]\tLoss: 0.691869\n",
            "Train Epoch: 1 [9856/10089 (97%)]\tLoss: 0.678819\n",
            "Train Epoch: 1 [9920/10089 (98%)]\tLoss: 0.684355\n",
            "Train Epoch: 1 [9984/10089 (99%)]\tLoss: 0.848961\n",
            "Train Epoch: 1 [6437/10089 (99%)]\tLoss: 0.593821\n",
            "\n",
            " Train set: Average loss: 0.5959\n",
            "\n",
            "\n",
            " Validate set: Average loss: 0.5506\n",
            "\n",
            "Train Epoch: 2 [0/10089 (0%)]\tLoss: 0.631765\n",
            "Train Epoch: 2 [64/10089 (1%)]\tLoss: 0.491508\n",
            "Train Epoch: 2 [128/10089 (1%)]\tLoss: 0.578977\n",
            "Train Epoch: 2 [192/10089 (2%)]\tLoss: 0.536724\n",
            "Train Epoch: 2 [256/10089 (3%)]\tLoss: 0.528319\n",
            "Train Epoch: 2 [320/10089 (3%)]\tLoss: 0.592972\n",
            "Train Epoch: 2 [384/10089 (4%)]\tLoss: 0.555534\n",
            "Train Epoch: 2 [448/10089 (4%)]\tLoss: 0.538344\n",
            "Train Epoch: 2 [512/10089 (5%)]\tLoss: 0.582372\n",
            "Train Epoch: 2 [576/10089 (6%)]\tLoss: 0.594204\n",
            "Train Epoch: 2 [640/10089 (6%)]\tLoss: 0.573663\n",
            "Train Epoch: 2 [704/10089 (7%)]\tLoss: 0.633604\n",
            "Train Epoch: 2 [768/10089 (8%)]\tLoss: 0.491232\n",
            "Train Epoch: 2 [832/10089 (8%)]\tLoss: 0.603066\n",
            "Train Epoch: 2 [896/10089 (9%)]\tLoss: 0.703091\n",
            "Train Epoch: 2 [960/10089 (9%)]\tLoss: 0.503475\n",
            "Train Epoch: 2 [1024/10089 (10%)]\tLoss: 0.583276\n",
            "Train Epoch: 2 [1088/10089 (11%)]\tLoss: 0.532682\n",
            "Train Epoch: 2 [1152/10089 (11%)]\tLoss: 0.533035\n",
            "Train Epoch: 2 [1216/10089 (12%)]\tLoss: 0.636806\n",
            "Train Epoch: 2 [1280/10089 (13%)]\tLoss: 0.622815\n",
            "Train Epoch: 2 [1344/10089 (13%)]\tLoss: 0.639130\n",
            "Train Epoch: 2 [1408/10089 (14%)]\tLoss: 0.516550\n",
            "Train Epoch: 2 [1472/10089 (15%)]\tLoss: 0.565299\n",
            "Train Epoch: 2 [1536/10089 (15%)]\tLoss: 0.504962\n",
            "Train Epoch: 2 [1600/10089 (16%)]\tLoss: 0.542217\n",
            "Train Epoch: 2 [1664/10089 (16%)]\tLoss: 0.596188\n",
            "Train Epoch: 2 [1728/10089 (17%)]\tLoss: 0.559532\n",
            "Train Epoch: 2 [1792/10089 (18%)]\tLoss: 0.672331\n",
            "Train Epoch: 2 [1856/10089 (18%)]\tLoss: 0.583443\n",
            "Train Epoch: 2 [1920/10089 (19%)]\tLoss: 0.654561\n",
            "Train Epoch: 2 [1984/10089 (20%)]\tLoss: 0.746691\n",
            "Train Epoch: 2 [2048/10089 (20%)]\tLoss: 0.673369\n",
            "Train Epoch: 2 [2112/10089 (21%)]\tLoss: 0.609934\n",
            "Train Epoch: 2 [2176/10089 (22%)]\tLoss: 0.720533\n",
            "Train Epoch: 2 [2240/10089 (22%)]\tLoss: 0.576845\n",
            "Train Epoch: 2 [2304/10089 (23%)]\tLoss: 0.677502\n",
            "Train Epoch: 2 [2368/10089 (23%)]\tLoss: 0.496668\n",
            "Train Epoch: 2 [2432/10089 (24%)]\tLoss: 0.567142\n",
            "Train Epoch: 2 [2496/10089 (25%)]\tLoss: 0.499992\n",
            "Train Epoch: 2 [2560/10089 (25%)]\tLoss: 0.553900\n",
            "Train Epoch: 2 [2624/10089 (26%)]\tLoss: 0.576185\n",
            "Train Epoch: 2 [2688/10089 (27%)]\tLoss: 0.526718\n",
            "Train Epoch: 2 [2752/10089 (27%)]\tLoss: 0.541012\n",
            "Train Epoch: 2 [2816/10089 (28%)]\tLoss: 0.587682\n",
            "Train Epoch: 2 [2880/10089 (28%)]\tLoss: 0.592297\n",
            "Train Epoch: 2 [2944/10089 (29%)]\tLoss: 0.646792\n",
            "Train Epoch: 2 [3008/10089 (30%)]\tLoss: 0.598569\n",
            "Train Epoch: 2 [3072/10089 (30%)]\tLoss: 0.555195\n",
            "Train Epoch: 2 [3136/10089 (31%)]\tLoss: 0.555834\n",
            "Train Epoch: 2 [3200/10089 (32%)]\tLoss: 0.522556\n",
            "Train Epoch: 2 [3264/10089 (32%)]\tLoss: 0.671561\n",
            "Train Epoch: 2 [3328/10089 (33%)]\tLoss: 0.621495\n",
            "Train Epoch: 2 [3392/10089 (34%)]\tLoss: 0.642967\n",
            "Train Epoch: 2 [3456/10089 (34%)]\tLoss: 0.668556\n",
            "Train Epoch: 2 [3520/10089 (35%)]\tLoss: 0.603266\n",
            "Train Epoch: 2 [3584/10089 (35%)]\tLoss: 0.521566\n",
            "Train Epoch: 2 [3648/10089 (36%)]\tLoss: 0.658233\n",
            "Train Epoch: 2 [3712/10089 (37%)]\tLoss: 0.765832\n",
            "Train Epoch: 2 [3776/10089 (37%)]\tLoss: 0.616248\n",
            "Train Epoch: 2 [3840/10089 (38%)]\tLoss: 0.659217\n",
            "Train Epoch: 2 [3904/10089 (39%)]\tLoss: 0.598411\n",
            "Train Epoch: 2 [3968/10089 (39%)]\tLoss: 0.549999\n",
            "Train Epoch: 2 [4032/10089 (40%)]\tLoss: 0.618350\n",
            "Train Epoch: 2 [4096/10089 (41%)]\tLoss: 0.489791\n",
            "Train Epoch: 2 [4160/10089 (41%)]\tLoss: 0.568853\n",
            "Train Epoch: 2 [4224/10089 (42%)]\tLoss: 0.534436\n",
            "Train Epoch: 2 [4288/10089 (42%)]\tLoss: 0.651114\n",
            "Train Epoch: 2 [4352/10089 (43%)]\tLoss: 0.585712\n",
            "Train Epoch: 2 [4416/10089 (44%)]\tLoss: 0.546713\n",
            "Train Epoch: 2 [4480/10089 (44%)]\tLoss: 0.510615\n",
            "Train Epoch: 2 [4544/10089 (45%)]\tLoss: 0.599004\n",
            "Train Epoch: 2 [4608/10089 (46%)]\tLoss: 0.532714\n",
            "Train Epoch: 2 [4672/10089 (46%)]\tLoss: 0.553810\n",
            "Train Epoch: 2 [4736/10089 (47%)]\tLoss: 0.524512\n",
            "Train Epoch: 2 [4800/10089 (47%)]\tLoss: 0.550080\n",
            "Train Epoch: 2 [4864/10089 (48%)]\tLoss: 0.568028\n",
            "Train Epoch: 2 [4928/10089 (49%)]\tLoss: 0.525117\n",
            "Train Epoch: 2 [4992/10089 (49%)]\tLoss: 0.652620\n",
            "Train Epoch: 2 [5056/10089 (50%)]\tLoss: 0.573914\n",
            "Train Epoch: 2 [5120/10089 (51%)]\tLoss: 0.556287\n",
            "Train Epoch: 2 [5184/10089 (51%)]\tLoss: 0.644010\n",
            "Train Epoch: 2 [5248/10089 (52%)]\tLoss: 0.508334\n",
            "Train Epoch: 2 [5312/10089 (53%)]\tLoss: 0.569418\n",
            "Train Epoch: 2 [5376/10089 (53%)]\tLoss: 0.593593\n",
            "Train Epoch: 2 [5440/10089 (54%)]\tLoss: 0.514280\n",
            "Train Epoch: 2 [5504/10089 (54%)]\tLoss: 0.624186\n",
            "Train Epoch: 2 [5568/10089 (55%)]\tLoss: 0.490557\n",
            "Train Epoch: 2 [5632/10089 (56%)]\tLoss: 0.618659\n",
            "Train Epoch: 2 [5696/10089 (56%)]\tLoss: 0.673528\n",
            "Train Epoch: 2 [5760/10089 (57%)]\tLoss: 0.563327\n",
            "Train Epoch: 2 [5824/10089 (58%)]\tLoss: 0.520129\n",
            "Train Epoch: 2 [5888/10089 (58%)]\tLoss: 0.610654\n",
            "Train Epoch: 2 [5952/10089 (59%)]\tLoss: 0.486886\n",
            "Train Epoch: 2 [6016/10089 (59%)]\tLoss: 0.568932\n",
            "Train Epoch: 2 [6080/10089 (60%)]\tLoss: 0.362869\n",
            "Train Epoch: 2 [6144/10089 (61%)]\tLoss: 0.588319\n",
            "Train Epoch: 2 [6208/10089 (61%)]\tLoss: 0.554926\n",
            "Train Epoch: 2 [6272/10089 (62%)]\tLoss: 0.497483\n",
            "Train Epoch: 2 [6336/10089 (63%)]\tLoss: 0.549207\n",
            "Train Epoch: 2 [6400/10089 (63%)]\tLoss: 0.549716\n",
            "Train Epoch: 2 [6464/10089 (64%)]\tLoss: 0.694142\n",
            "Train Epoch: 2 [6528/10089 (65%)]\tLoss: 0.600709\n",
            "Train Epoch: 2 [6592/10089 (65%)]\tLoss: 0.596305\n",
            "Train Epoch: 2 [6656/10089 (66%)]\tLoss: 0.550849\n",
            "Train Epoch: 2 [6720/10089 (66%)]\tLoss: 0.708959\n",
            "Train Epoch: 2 [6784/10089 (67%)]\tLoss: 0.597064\n",
            "Train Epoch: 2 [6848/10089 (68%)]\tLoss: 0.610432\n",
            "Train Epoch: 2 [6912/10089 (68%)]\tLoss: 0.503785\n",
            "Train Epoch: 2 [6976/10089 (69%)]\tLoss: 0.579361\n",
            "Train Epoch: 2 [7040/10089 (70%)]\tLoss: 0.535514\n",
            "Train Epoch: 2 [7104/10089 (70%)]\tLoss: 0.735452\n",
            "Train Epoch: 2 [7168/10089 (71%)]\tLoss: 0.654603\n",
            "Train Epoch: 2 [7232/10089 (72%)]\tLoss: 0.520258\n",
            "Train Epoch: 2 [7296/10089 (72%)]\tLoss: 0.608999\n",
            "Train Epoch: 2 [7360/10089 (73%)]\tLoss: 0.579568\n",
            "Train Epoch: 2 [7424/10089 (73%)]\tLoss: 0.465704\n",
            "Train Epoch: 2 [7488/10089 (74%)]\tLoss: 0.585093\n",
            "Train Epoch: 2 [7552/10089 (75%)]\tLoss: 0.542916\n",
            "Train Epoch: 2 [7616/10089 (75%)]\tLoss: 0.552031\n",
            "Train Epoch: 2 [7680/10089 (76%)]\tLoss: 0.546519\n",
            "Train Epoch: 2 [7744/10089 (77%)]\tLoss: 0.670923\n",
            "Train Epoch: 2 [7808/10089 (77%)]\tLoss: 0.580937\n",
            "Train Epoch: 2 [7872/10089 (78%)]\tLoss: 0.543252\n",
            "Train Epoch: 2 [7936/10089 (78%)]\tLoss: 0.547222\n",
            "Train Epoch: 2 [8000/10089 (79%)]\tLoss: 0.519025\n",
            "Train Epoch: 2 [8064/10089 (80%)]\tLoss: 0.507883\n",
            "Train Epoch: 2 [8128/10089 (80%)]\tLoss: 0.627861\n",
            "Train Epoch: 2 [8192/10089 (81%)]\tLoss: 0.673723\n",
            "Train Epoch: 2 [8256/10089 (82%)]\tLoss: 0.559757\n",
            "Train Epoch: 2 [8320/10089 (82%)]\tLoss: 0.491308\n",
            "Train Epoch: 2 [8384/10089 (83%)]\tLoss: 0.540817\n",
            "Train Epoch: 2 [8448/10089 (84%)]\tLoss: 0.593382\n",
            "Train Epoch: 2 [8512/10089 (84%)]\tLoss: 0.535401\n",
            "Train Epoch: 2 [8576/10089 (85%)]\tLoss: 0.395550\n",
            "Train Epoch: 2 [8640/10089 (85%)]\tLoss: 0.546929\n",
            "Train Epoch: 2 [8704/10089 (86%)]\tLoss: 0.547515\n",
            "Train Epoch: 2 [8768/10089 (87%)]\tLoss: 0.567416\n",
            "Train Epoch: 2 [8832/10089 (87%)]\tLoss: 0.578218\n",
            "Train Epoch: 2 [8896/10089 (88%)]\tLoss: 0.534147\n",
            "Train Epoch: 2 [8960/10089 (89%)]\tLoss: 0.525078\n",
            "Train Epoch: 2 [9024/10089 (89%)]\tLoss: 0.533440\n",
            "Train Epoch: 2 [9088/10089 (90%)]\tLoss: 0.561207\n",
            "Train Epoch: 2 [9152/10089 (91%)]\tLoss: 0.632105\n",
            "Train Epoch: 2 [9216/10089 (91%)]\tLoss: 0.713538\n",
            "Train Epoch: 2 [9280/10089 (92%)]\tLoss: 0.557571\n",
            "Train Epoch: 2 [9344/10089 (92%)]\tLoss: 0.469731\n",
            "Train Epoch: 2 [9408/10089 (93%)]\tLoss: 0.471721\n",
            "Train Epoch: 2 [9472/10089 (94%)]\tLoss: 0.507089\n",
            "Train Epoch: 2 [9536/10089 (94%)]\tLoss: 0.551247\n",
            "Train Epoch: 2 [9600/10089 (95%)]\tLoss: 0.613597\n",
            "Train Epoch: 2 [9664/10089 (96%)]\tLoss: 0.469249\n",
            "Train Epoch: 2 [9728/10089 (96%)]\tLoss: 0.516314\n",
            "Train Epoch: 2 [9792/10089 (97%)]\tLoss: 0.538008\n",
            "Train Epoch: 2 [9856/10089 (97%)]\tLoss: 0.724232\n",
            "Train Epoch: 2 [9920/10089 (98%)]\tLoss: 0.560766\n",
            "Train Epoch: 2 [9984/10089 (99%)]\tLoss: 0.597217\n",
            "Train Epoch: 2 [6437/10089 (99%)]\tLoss: 0.637146\n",
            "\n",
            " Train set: Average loss: 0.6441\n",
            "\n",
            "\n",
            " Validate set: Average loss: 0.6269\n",
            "\n",
            "Train Epoch: 3 [0/10089 (0%)]\tLoss: 0.609187\n",
            "Train Epoch: 3 [64/10089 (1%)]\tLoss: 0.526250\n",
            "Train Epoch: 3 [128/10089 (1%)]\tLoss: 0.609056\n",
            "Train Epoch: 3 [192/10089 (2%)]\tLoss: 0.507018\n",
            "Train Epoch: 3 [256/10089 (3%)]\tLoss: 0.629266\n",
            "Train Epoch: 3 [320/10089 (3%)]\tLoss: 0.463021\n",
            "Train Epoch: 3 [384/10089 (4%)]\tLoss: 0.576778\n",
            "Train Epoch: 3 [448/10089 (4%)]\tLoss: 0.498386\n",
            "Train Epoch: 3 [512/10089 (5%)]\tLoss: 0.554084\n",
            "Train Epoch: 3 [576/10089 (6%)]\tLoss: 0.616143\n",
            "Train Epoch: 3 [640/10089 (6%)]\tLoss: 0.573998\n",
            "Train Epoch: 3 [704/10089 (7%)]\tLoss: 0.564013\n",
            "Train Epoch: 3 [768/10089 (8%)]\tLoss: 0.508792\n",
            "Train Epoch: 3 [832/10089 (8%)]\tLoss: 0.526977\n",
            "Train Epoch: 3 [896/10089 (9%)]\tLoss: 0.696838\n",
            "Train Epoch: 3 [960/10089 (9%)]\tLoss: 0.508451\n",
            "Train Epoch: 3 [1024/10089 (10%)]\tLoss: 0.518130\n",
            "Train Epoch: 3 [1088/10089 (11%)]\tLoss: 0.502663\n",
            "Train Epoch: 3 [1152/10089 (11%)]\tLoss: 0.587842\n",
            "Train Epoch: 3 [1216/10089 (12%)]\tLoss: 0.699815\n",
            "Train Epoch: 3 [1280/10089 (13%)]\tLoss: 0.506414\n",
            "Train Epoch: 3 [1344/10089 (13%)]\tLoss: 0.530008\n",
            "Train Epoch: 3 [1408/10089 (14%)]\tLoss: 0.502220\n",
            "Train Epoch: 3 [1472/10089 (15%)]\tLoss: 0.579033\n",
            "Train Epoch: 3 [1536/10089 (15%)]\tLoss: 0.496524\n",
            "Train Epoch: 3 [1600/10089 (16%)]\tLoss: 0.576396\n",
            "Train Epoch: 3 [1664/10089 (16%)]\tLoss: 0.467412\n",
            "Train Epoch: 3 [1728/10089 (17%)]\tLoss: 0.484793\n",
            "Train Epoch: 3 [1792/10089 (18%)]\tLoss: 0.505492\n",
            "Train Epoch: 3 [1856/10089 (18%)]\tLoss: 0.532586\n",
            "Train Epoch: 3 [1920/10089 (19%)]\tLoss: 0.545236\n",
            "Train Epoch: 3 [1984/10089 (20%)]\tLoss: 0.541554\n",
            "Train Epoch: 3 [2048/10089 (20%)]\tLoss: 0.512362\n",
            "Train Epoch: 3 [2112/10089 (21%)]\tLoss: 0.685951\n",
            "Train Epoch: 3 [2176/10089 (22%)]\tLoss: 0.542862\n",
            "Train Epoch: 3 [2240/10089 (22%)]\tLoss: 0.629261\n",
            "Train Epoch: 3 [2304/10089 (23%)]\tLoss: 0.566550\n",
            "Train Epoch: 3 [2368/10089 (23%)]\tLoss: 0.500316\n",
            "Train Epoch: 3 [2432/10089 (24%)]\tLoss: 0.507684\n",
            "Train Epoch: 3 [2496/10089 (25%)]\tLoss: 0.702013\n",
            "Train Epoch: 3 [2560/10089 (25%)]\tLoss: 0.549693\n",
            "Train Epoch: 3 [2624/10089 (26%)]\tLoss: 0.554219\n",
            "Train Epoch: 3 [2688/10089 (27%)]\tLoss: 0.539265\n",
            "Train Epoch: 3 [2752/10089 (27%)]\tLoss: 0.535763\n",
            "Train Epoch: 3 [2816/10089 (28%)]\tLoss: 0.582873\n",
            "Train Epoch: 3 [2880/10089 (28%)]\tLoss: 0.524405\n",
            "Train Epoch: 3 [2944/10089 (29%)]\tLoss: 0.600319\n",
            "Train Epoch: 3 [3008/10089 (30%)]\tLoss: 0.523694\n",
            "Train Epoch: 3 [3072/10089 (30%)]\tLoss: 0.468987\n",
            "Train Epoch: 3 [3136/10089 (31%)]\tLoss: 0.602934\n",
            "Train Epoch: 3 [3200/10089 (32%)]\tLoss: 0.571212\n",
            "Train Epoch: 3 [3264/10089 (32%)]\tLoss: 0.406406\n",
            "Train Epoch: 3 [3328/10089 (33%)]\tLoss: 0.578180\n",
            "Train Epoch: 3 [3392/10089 (34%)]\tLoss: 0.454358\n",
            "Train Epoch: 3 [3456/10089 (34%)]\tLoss: 0.526127\n",
            "Train Epoch: 3 [3520/10089 (35%)]\tLoss: 0.571865\n",
            "Train Epoch: 3 [3584/10089 (35%)]\tLoss: 0.536602\n",
            "Train Epoch: 3 [3648/10089 (36%)]\tLoss: 0.541839\n",
            "Train Epoch: 3 [3712/10089 (37%)]\tLoss: 0.459229\n",
            "Train Epoch: 3 [3776/10089 (37%)]\tLoss: 0.528908\n",
            "Train Epoch: 3 [3840/10089 (38%)]\tLoss: 0.543033\n",
            "Train Epoch: 3 [3904/10089 (39%)]\tLoss: 0.513408\n",
            "Train Epoch: 3 [3968/10089 (39%)]\tLoss: 0.551643\n",
            "Train Epoch: 3 [4032/10089 (40%)]\tLoss: 0.516388\n",
            "Train Epoch: 3 [4096/10089 (41%)]\tLoss: 0.519146\n",
            "Train Epoch: 3 [4160/10089 (41%)]\tLoss: 0.544649\n",
            "Train Epoch: 3 [4224/10089 (42%)]\tLoss: 0.505005\n",
            "Train Epoch: 3 [4288/10089 (42%)]\tLoss: 0.510668\n",
            "Train Epoch: 3 [4352/10089 (43%)]\tLoss: 0.508788\n",
            "Train Epoch: 3 [4416/10089 (44%)]\tLoss: 0.506103\n",
            "Train Epoch: 3 [4480/10089 (44%)]\tLoss: 0.588116\n",
            "Train Epoch: 3 [4544/10089 (45%)]\tLoss: 0.509158\n",
            "Train Epoch: 3 [4608/10089 (46%)]\tLoss: 0.629312\n",
            "Train Epoch: 3 [4672/10089 (46%)]\tLoss: 0.472318\n",
            "Train Epoch: 3 [4736/10089 (47%)]\tLoss: 0.418078\n",
            "Train Epoch: 3 [4800/10089 (47%)]\tLoss: 0.575201\n",
            "Train Epoch: 3 [4864/10089 (48%)]\tLoss: 0.558203\n",
            "Train Epoch: 3 [4928/10089 (49%)]\tLoss: 0.457907\n",
            "Train Epoch: 3 [4992/10089 (49%)]\tLoss: 0.495094\n",
            "Train Epoch: 3 [5056/10089 (50%)]\tLoss: 0.477453\n",
            "Train Epoch: 3 [5120/10089 (51%)]\tLoss: 0.501632\n",
            "Train Epoch: 3 [5184/10089 (51%)]\tLoss: 0.455061\n",
            "Train Epoch: 3 [5248/10089 (52%)]\tLoss: 0.525524\n",
            "Train Epoch: 3 [5312/10089 (53%)]\tLoss: 0.527955\n",
            "Train Epoch: 3 [5376/10089 (53%)]\tLoss: 0.467021\n",
            "Train Epoch: 3 [5440/10089 (54%)]\tLoss: 0.550546\n",
            "Train Epoch: 3 [5504/10089 (54%)]\tLoss: 0.501789\n",
            "Train Epoch: 3 [5568/10089 (55%)]\tLoss: 0.518473\n",
            "Train Epoch: 3 [5632/10089 (56%)]\tLoss: 0.482940\n",
            "Train Epoch: 3 [5696/10089 (56%)]\tLoss: 0.480639\n",
            "Train Epoch: 3 [5760/10089 (57%)]\tLoss: 0.538044\n",
            "Train Epoch: 3 [5824/10089 (58%)]\tLoss: 0.634039\n",
            "Train Epoch: 3 [5888/10089 (58%)]\tLoss: 0.614109\n",
            "Train Epoch: 3 [5952/10089 (59%)]\tLoss: 0.539205\n",
            "Train Epoch: 3 [6016/10089 (59%)]\tLoss: 0.468791\n",
            "Train Epoch: 3 [6080/10089 (60%)]\tLoss: 0.536934\n",
            "Train Epoch: 3 [6144/10089 (61%)]\tLoss: 0.471248\n",
            "Train Epoch: 3 [6208/10089 (61%)]\tLoss: 0.515523\n",
            "Train Epoch: 3 [6272/10089 (62%)]\tLoss: 0.592009\n",
            "Train Epoch: 3 [6336/10089 (63%)]\tLoss: 0.540698\n",
            "Train Epoch: 3 [6400/10089 (63%)]\tLoss: 0.579944\n",
            "Train Epoch: 3 [6464/10089 (64%)]\tLoss: 0.547549\n",
            "Train Epoch: 3 [6528/10089 (65%)]\tLoss: 0.626087\n",
            "Train Epoch: 3 [6592/10089 (65%)]\tLoss: 0.568563\n",
            "Train Epoch: 3 [6656/10089 (66%)]\tLoss: 0.501762\n",
            "Train Epoch: 3 [6720/10089 (66%)]\tLoss: 0.542092\n",
            "Train Epoch: 3 [6784/10089 (67%)]\tLoss: 0.559002\n",
            "Train Epoch: 3 [6848/10089 (68%)]\tLoss: 0.547893\n",
            "Train Epoch: 3 [6912/10089 (68%)]\tLoss: 0.464247\n",
            "Train Epoch: 3 [6976/10089 (69%)]\tLoss: 0.515805\n",
            "Train Epoch: 3 [7040/10089 (70%)]\tLoss: 0.570486\n",
            "Train Epoch: 3 [7104/10089 (70%)]\tLoss: 0.536318\n",
            "Train Epoch: 3 [7168/10089 (71%)]\tLoss: 0.499058\n",
            "Train Epoch: 3 [7232/10089 (72%)]\tLoss: 0.490705\n",
            "Train Epoch: 3 [7296/10089 (72%)]\tLoss: 0.586384\n",
            "Train Epoch: 3 [7360/10089 (73%)]\tLoss: 0.549705\n",
            "Train Epoch: 3 [7424/10089 (73%)]\tLoss: 0.506708\n",
            "Train Epoch: 3 [7488/10089 (74%)]\tLoss: 0.615447\n",
            "Train Epoch: 3 [7552/10089 (75%)]\tLoss: 0.422619\n",
            "Train Epoch: 3 [7616/10089 (75%)]\tLoss: 0.484768\n",
            "Train Epoch: 3 [7680/10089 (76%)]\tLoss: 0.478099\n",
            "Train Epoch: 3 [7744/10089 (77%)]\tLoss: 0.494414\n",
            "Train Epoch: 3 [7808/10089 (77%)]\tLoss: 0.477391\n",
            "Train Epoch: 3 [7872/10089 (78%)]\tLoss: 0.442219\n",
            "Train Epoch: 3 [7936/10089 (78%)]\tLoss: 0.461077\n",
            "Train Epoch: 3 [8000/10089 (79%)]\tLoss: 0.485112\n",
            "Train Epoch: 3 [8064/10089 (80%)]\tLoss: 0.551314\n",
            "Train Epoch: 3 [8128/10089 (80%)]\tLoss: 0.567842\n",
            "Train Epoch: 3 [8192/10089 (81%)]\tLoss: 0.506190\n",
            "Train Epoch: 3 [8256/10089 (82%)]\tLoss: 0.382490\n",
            "Train Epoch: 3 [8320/10089 (82%)]\tLoss: 0.485082\n",
            "Train Epoch: 3 [8384/10089 (83%)]\tLoss: 0.459202\n",
            "Train Epoch: 3 [8448/10089 (84%)]\tLoss: 0.517796\n",
            "Train Epoch: 3 [8512/10089 (84%)]\tLoss: 0.476603\n",
            "Train Epoch: 3 [8576/10089 (85%)]\tLoss: 0.547906\n",
            "Train Epoch: 3 [8640/10089 (85%)]\tLoss: 0.444672\n",
            "Train Epoch: 3 [8704/10089 (86%)]\tLoss: 0.507853\n",
            "Train Epoch: 3 [8768/10089 (87%)]\tLoss: 0.485995\n",
            "Train Epoch: 3 [8832/10089 (87%)]\tLoss: 0.475010\n",
            "Train Epoch: 3 [8896/10089 (88%)]\tLoss: 0.454580\n",
            "Train Epoch: 3 [8960/10089 (89%)]\tLoss: 0.578608\n",
            "Train Epoch: 3 [9024/10089 (89%)]\tLoss: 0.483843\n",
            "Train Epoch: 3 [9088/10089 (90%)]\tLoss: 0.428874\n",
            "Train Epoch: 3 [9152/10089 (91%)]\tLoss: 0.564527\n",
            "Train Epoch: 3 [9216/10089 (91%)]\tLoss: 0.450585\n",
            "Train Epoch: 3 [9280/10089 (92%)]\tLoss: 0.657392\n",
            "Train Epoch: 3 [9344/10089 (92%)]\tLoss: 0.487052\n",
            "Train Epoch: 3 [9408/10089 (93%)]\tLoss: 0.423158\n",
            "Train Epoch: 3 [9472/10089 (94%)]\tLoss: 0.500072\n",
            "Train Epoch: 3 [9536/10089 (94%)]\tLoss: 0.604524\n",
            "Train Epoch: 3 [9600/10089 (95%)]\tLoss: 0.474170\n",
            "Train Epoch: 3 [9664/10089 (96%)]\tLoss: 0.607023\n",
            "Train Epoch: 3 [9728/10089 (96%)]\tLoss: 0.594502\n",
            "Train Epoch: 3 [9792/10089 (97%)]\tLoss: 0.507692\n",
            "Train Epoch: 3 [9856/10089 (97%)]\tLoss: 0.512038\n",
            "Train Epoch: 3 [9920/10089 (98%)]\tLoss: 0.517589\n",
            "Train Epoch: 3 [9984/10089 (99%)]\tLoss: 0.514547\n",
            "Train Epoch: 3 [6437/10089 (99%)]\tLoss: 0.481062\n",
            "\n",
            " Train set: Average loss: 0.4981\n",
            "\n",
            "\n",
            " Validate set: Average loss: 0.4693\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss = []\n",
        "train_loss = []\n",
        "train_loss.np.array(train_loss)\n",
        "val_loss.np.array(val_loss)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(val_loss,label=\"validation\")\n",
        "plt.plot(train_loss,label=\"train\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KMJPq0FWQg_d",
        "outputId": "d0217927-ab16-4210-9d2e-ca0484b8e510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2d514b72590c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'np'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots( figsize=(20,10))\n",
        "ax.plot(train_loss.train_loss['train_loss'])\n",
        "ax.plot(val_loss.val_loss['val_loss'])\n",
        "plt.title('Model Error')\n",
        "plt.ylabel('error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "ax.grid(color='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QDrIq0zHtBcv",
        "outputId": "31926aec-e3d2-4924-9e8d-d5b6595f8049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a7b60a324cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'train_loss'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check = torch.rand(5,1,256,256)\n",
        "# model = BoneAgePredictor();\n",
        "# print(model.forward(check).size())"
      ],
      "metadata": {
        "id": "Y7ycrSgtQtp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check = pd.read_csv(dataset_path('boneage-training-dataset.csv'));\n",
        "# print(check)\n",
        "# print(check['boneage'][0])\n",
        "# print(check['id'][1])\n",
        "# check = Image.open(dataset_path('boneage-training-dataset',str(check['id'][1])+'.png')).resize((256,256))\n",
        "# print(check)\n",
        "# # check = transform(check)\n",
        "# # print(check.size())\n",
        "# # check = torch.from_numpy(np.array(check['boneage'][0]))\n",
        "# # print(check)"
      ],
      "metadata": {
        "id": "ces1HsnPbNtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}